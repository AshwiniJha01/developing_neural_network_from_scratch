{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris() #loading a sample data for checking code errors\n",
    "X = data.data[:, :4] # saving independent variables\n",
    "y = data.target # saving the dependent variables\n",
    "y[y == 2] = 1 # converting three class into two class\n",
    "X = np.asmatrix(X) # converting into matrix\n",
    "X = np.transpose(X) # tranposing the matrix to have all value of one record in one column and all records as different columns\n",
    "y = np.asmatrix(y) #converting into matrix\n",
    "#numLayers = 3 # number of layers in the neural network\n",
    "#numNodes = [3, 2, 1] # number of nodes in each layer\n",
    "#alpha = 0.01 # learning rate in gradient descent\n",
    "#epochs = 10 # number of forward and backward passes through the entire training data\n",
    "#costVal = [None]*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the sigmoid function for activation\n",
    "def sigmoidFun(z):\n",
    "    op = 1/(1 + np.exp(-z))\n",
    "    return(op)\n",
    "\n",
    "# defining the cost calculation function\n",
    "def costFun(yActual, yPredicted):\n",
    "    lossValues = [None]*np.shape(yActual)[1]\n",
    "    logY0 = np.log(yPredicted)\n",
    "    logY1 = np.log(1 - yPredicted)\n",
    "    for i in range(np.shape(yActual)[1]):\n",
    "        lossValues[i] = -(yActual.item((0,i))*logY0.item((0,i)) + (1-yActual.item((0,i)))*logY1.item((0,i)))\n",
    "    costValue = np.sum(lossValues)/np.shape(yActual)[1]\n",
    "    return(costValue)\n",
    "\n",
    "# defining derivative of the sigmoid function\n",
    "def sigmoidDer(z): # Derivative of the sigmoid function sigmoid(z)*(1-sigmoid(z)):\n",
    "    op = np.array(sigmoidFun(z))*np.array((1-sigmoidFun(z)))\n",
    "    return(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the weights and biases of the linear function of each node\n",
    "W = [None]*numLayers\n",
    "B = [None]*numLayers\n",
    "for i in range(numLayers):\n",
    "    if i == 0:\n",
    "        W[i] = np.asmatrix(np.random.rand(numNodes[i],np.shape(X)[0]))\n",
    "        B[i] = np.asmatrix(np.random.rand(numNodes[i],1))\n",
    "        continue\n",
    "    W[i] = np.asmatrix(np.random.rand(numNodes[i],np.shape(W[i-1])[0]))\n",
    "    B[i] = np.asmatrix(np.random.rand(numNodes[i],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Z (linear output W*X + B) values and the activation function (sigmoid(Z)) values for each node, this makes the forward propagation\n",
    "Z = [None]*numLayers\n",
    "Sig = [None]*numLayers\n",
    "for i in range(numLayers):\n",
    "    if i == 0:\n",
    "        Z[i] = np.dot(W[i],X) + B[i]\n",
    "        Sig[i] = sigmoidFun(Z[i])\n",
    "        continue\n",
    "    Z[i] = np.dot(W[i], Sig[i - 1]) + B[i]\n",
    "    Sig[i] = sigmoidFun(Z[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the loss across the output:\n",
    "costVal[0] = costFun(yActual = y, yPredicted = Sig[(numLayers - 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the derivatives for the backward propagation:\n",
    "dZ = [None]*numLayers\n",
    "dW = [None]*numLayers\n",
    "dB = [None]*numLayers\n",
    "for i in range(numLayers, 0, -1):\n",
    "    if i == 1:\n",
    "        dZ[i-1] = Sig[i-1] - y\n",
    "        dW[i-1] = (1/np.shape(X)[1])*(dZ[i-1])*np.transpose(X)\n",
    "        dB[i-1] = (1/np.shape(X)[1])*np.sum(dZ[i-1], axis = 1)\n",
    "        continue\n",
    "    dZ[i-1] = Sig[i-1] - y\n",
    "    dW[i-1] = (1/np.shape(X)[1])*(dZ[i-1])*np.transpose(Sig[i-2])\n",
    "    dB[i-1] = (1/np.shape(X)[1])*np.sum(dZ[i-1], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the weights and biases:\n",
    "for i in range(numLayers):\n",
    "    W[i] = W[i] - alpha*dW[i]\n",
    "    B[i] = B[i] - alpha*dB[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defining the sigmoid function for activation\n",
    "def sigmoidFun(z):\n",
    "    op = 1/(1 + np.exp(-z))\n",
    "    return(op)\n",
    "\n",
    "# defining the cost calculation function\n",
    "def costFun(yActual, yPredicted):\n",
    "    lossValues = [None]*np.shape(yActual)[1]\n",
    "    logY0 = np.log(yPredicted)\n",
    "    logY1 = np.log(1 - yPredicted)\n",
    "    for i in range(np.shape(yActual)[1]):\n",
    "        lossValues[i] = -(yActual.item((0,i))*logY0.item((0,i)) + (1-yActual.item((0,i)))*logY1.item((0,i)))\n",
    "    costValue = np.sum(lossValues)/np.shape(yActual)[1]\n",
    "    return(costValue)\n",
    "\n",
    "# defining derivative of the sigmoid function\n",
    "def sigmoidDer(z): # Derivative of the sigmoid function sigmoid(z)*(1-sigmoid(z)):\n",
    "    op = np.array(sigmoidFun(z))*np.array((1-sigmoidFun(z)))\n",
    "    return(op)\n",
    "\n",
    "\n",
    "# Neural Network function that will be able to do above steps for epoch number of times and return final weights, biases and cost value for each function:\n",
    "def neuralNet(X, y, numLayers, numNodes, alpha, epochs):\n",
    "    \n",
    "    costVal = [None]*epochs # Creating array to store cost value from each epoch\n",
    "    \n",
    "    # Initializing the weights and biases of the linear function of each node\n",
    "    W = [None]*numLayers\n",
    "    B = [None]*numLayers\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            W[i] = np.asmatrix(np.random.rand(numNodes[i],np.shape(X)[0]))\n",
    "            B[i] = np.asmatrix(np.random.rand(numNodes[i],1))\n",
    "            continue\n",
    "        W[i] = np.asmatrix(np.random.rand(numNodes[i],np.shape(W[i-1])[0]))\n",
    "        B[i] = np.asmatrix(np.random.rand(numNodes[i],1))\n",
    "    \n",
    "    for k in range(epochs):\n",
    "        \n",
    "        # Calculating the Z (linear output W*X + B) values and the activation function (sigmoid(Z)) values for each node, this makes the forward propagation\n",
    "        Z = [None]*numLayers\n",
    "        Sig = [None]*numLayers\n",
    "        for i in range(numLayers):\n",
    "            if i == 0:\n",
    "                Z[i] = np.dot(W[i],X) + B[i]\n",
    "                Sig[i] = sigmoidFun(Z[i])\n",
    "                continue\n",
    "            Z[i] = np.dot(W[i], Sig[i - 1]) + B[i]\n",
    "            Sig[i] = sigmoidFun(Z[i])\n",
    "\n",
    "\n",
    "        # Calculating the loss across the output:\n",
    "        costVal[k] = costFun(yActual = y, yPredicted = Sig[(numLayers - 1)])\n",
    "\n",
    "\n",
    "        # Calculating the derivatives for the backward propagation:\n",
    "        dZ = [None]*numLayers\n",
    "        dW = [None]*numLayers\n",
    "        dB = [None]*numLayers\n",
    "        for i in range(numLayers, 0, -1):\n",
    "            if i == 1:\n",
    "                dZ[i-1] = Sig[i-1] - y\n",
    "                dW[i-1] = (1/np.shape(X)[1])*(dZ[i-1])*np.transpose(X)\n",
    "                dB[i-1] = (1/np.shape(X)[1])*np.sum(dZ[i-1], axis = 1)\n",
    "                continue\n",
    "            dZ[i-1] = Sig[i-1] - y\n",
    "            dW[i-1] = (1/np.shape(X)[1])*(dZ[i-1])*np.transpose(Sig[i-2])\n",
    "            dB[i-1] = (1/np.shape(X)[1])*np.sum(dZ[i-1], axis = 1)\n",
    "\n",
    "\n",
    "        # Updating the weights and biases:\n",
    "        for i in range(numLayers):\n",
    "            W[i] = W[i] - alpha*dW[i]\n",
    "            B[i] = B[i] - alpha*dB[i]\n",
    "\n",
    "\n",
    "    return [W, B, costVal, numLayers]\n",
    "\n",
    "# Function to predict using a neural network model:\n",
    "def predictNeuralNet(model, newData):\n",
    "    # Calculating the Z (linear output W*X + B) values and activation function (sigmoid(Z)) values for each node\n",
    "    numLayers = model[3]\n",
    "    W = model[0]\n",
    "    B = model[1]\n",
    "    Z = [None]*numLayers\n",
    "    Sig = [None]*numLayers\n",
    "    for i in range(numLayers):\n",
    "        if i == 0:\n",
    "            Z[i] = np.dot(W[i],newData) + B[i]\n",
    "            Sig[i] = sigmoidFun(Z[i])\n",
    "            continue\n",
    "        Z[i] = np.dot(W[i], Sig[i - 1]) + B[i]\n",
    "        Sig[i] = sigmoidFun(Z[i])\n",
    "    return(Sig[numLayers-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbd = neuralNet(X = X, y = y, numLayers = 3, numNodes = [3, 2, 1], alpha = 0.01, epochs = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.07752226, 0.07919542, 0.07820499, 0.08048099, 0.07744581,\n",
       "         0.07786951, 0.07859876, 0.07823859, 0.0813762 , 0.07913773,\n",
       "         0.07728296, 0.07926196, 0.07900522, 0.0780575 , 0.07666502,\n",
       "         0.07678609, 0.07693993, 0.07768817, 0.07758474, 0.07748614,\n",
       "         0.07876112, 0.07784334, 0.07692011, 0.08206033, 0.08350621,\n",
       "         0.08105581, 0.07969708, 0.07775915, 0.07760571, 0.08069836,\n",
       "         0.08106402, 0.07820529, 0.07692748, 0.07675799, 0.07958618,\n",
       "         0.07750149, 0.07706235, 0.07738241, 0.07953208, 0.07810242,\n",
       "         0.07746514, 0.08762722, 0.07868359, 0.08021616, 0.08025607,\n",
       "         0.07993122, 0.07763197, 0.07899031, 0.07734739, 0.07802369,\n",
       "         0.97838266, 0.97838669, 0.97842725, 0.97840941, 0.97842532,\n",
       "         0.97842681, 0.97841915, 0.97803814, 0.97840445, 0.97837735,\n",
       "         0.9783281 , 0.97837578, 0.97836335, 0.97843286, 0.97802094,\n",
       "         0.97833912, 0.97842847, 0.9783254 , 0.9784425 , 0.97833199,\n",
       "         0.97844217, 0.97828964, 0.97844777, 0.97842959, 0.97835238,\n",
       "         0.97836408, 0.9784302 , 0.97844336, 0.97842399, 0.97781746,\n",
       "         0.97832556, 0.9782513 , 0.97828281, 0.97845083, 0.97843224,\n",
       "         0.97839635, 0.97841324, 0.97842736, 0.9783421 , 0.97839249,\n",
       "         0.97842785, 0.97842077, 0.97834614, 0.9780723 , 0.97840273,\n",
       "         0.97834857, 0.97837718, 0.97836784, 0.97707176, 0.97836631,\n",
       "         0.97845395, 0.97845235, 0.97845355, 0.97845314, 0.97845373,\n",
       "         0.97845404, 0.97844925, 0.97845386, 0.97845374, 0.97845364,\n",
       "         0.97844814, 0.97845252, 0.97845265, 0.97845277, 0.97845316,\n",
       "         0.97845217, 0.97845229, 0.97845386, 0.97845414, 0.97845182,\n",
       "         0.97845323, 0.97845139, 0.97845408, 0.97844857, 0.9784529 ,\n",
       "         0.9784531 , 0.9784461 , 0.9784465 , 0.97845356, 0.9784523 ,\n",
       "         0.97845373, 0.97845303, 0.97845365, 0.97844839, 0.97845318,\n",
       "         0.97845375, 0.97845325, 0.97845214, 0.97844472, 0.9784516 ,\n",
       "         0.97845338, 0.97844965, 0.97845235, 0.97845368, 0.97845347,\n",
       "         0.9784517 , 0.97845154, 0.97845087, 0.97845241, 0.97845049]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictNeuralNet(model = tbd, newData = X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
